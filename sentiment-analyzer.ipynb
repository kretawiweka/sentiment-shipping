{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from googletrans import Translator, LANGUAGES\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "from googletrans import Translator, LANGUAGES\n",
    "np.seterr(divide = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "    def __init__(self):\n",
    "        def normalisasi(norm):\n",
    "            dt = {}\n",
    "            for i in range(len(norm)):\n",
    "                dt[norm.loc[i,\"kata typo\"]] = norm.loc[i,\"kata normal\"]\n",
    "            return dt\n",
    "        self.__kamusNorm = normalisasi(pd.read_csv(\"./data/components/word-normalization.csv\", delimiter=\";\"))\n",
    "        self.__stopword = list(pd.read_csv(\"./data/components/stopword.csv\",header=None).loc[:,0])\n",
    "\n",
    "    def token(self,text:list):\n",
    "        output = []\n",
    "        for i in range(len(text)):\n",
    "            output.append(text[i].split(\" \"))\n",
    "        return output\n",
    "    \n",
    "    def stem(self,text: list):\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "\n",
    "        output = []\n",
    "        # stemming process\n",
    "        for i in range(len(text)):\n",
    "            sentence = text[i]\n",
    "            is_list = False\n",
    "            if isinstance(sentence, list):\n",
    "                sentence = \" \".join(sentence)\n",
    "                is_list = True\n",
    "            temp   = stemmer.stem(sentence)\n",
    "            if is_list:\n",
    "                temp = temp.split(\" \")\n",
    "            output.append(temp)\n",
    "        return output\n",
    "\n",
    "    def filtering(self, text: list):\n",
    "        output = []\n",
    "        for i in range(len(text)):\n",
    "            temp = []\n",
    "            for j in range(len(text[i])):\n",
    "                if text[i][j] not in self.__stopword:\n",
    "                    temp.append(text[i][j])\n",
    "            output.append(temp)\n",
    "        return output\n",
    "    \n",
    "    def removeLink(self, data:list):\n",
    "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "        for i in range(len(data)):\n",
    "            data[i] = re.sub(regex, \" \", data[i])\n",
    "        return data\n",
    "    \n",
    "    def normalization(self,data):\n",
    "        length_data = np.array(data).shape\n",
    "        for i in range(length_data[0]):\n",
    "            is_str= isinstance(data[i], str)\n",
    "            temp = data[i]\n",
    "            if is_str:\n",
    "                temp = data[i].split(\" \")\n",
    "                \n",
    "            for j in range(len(temp)):\n",
    "                temp[j] = self.__kamusNorm.get(temp[j],temp[j])\n",
    "            if is_str:\n",
    "                temp = \" \".join(temp)\n",
    "            data[i] = temp\n",
    "        return data\n",
    "    \n",
    "    def prep_data(self,data):\n",
    "        data = self.removeLink(data)\n",
    "        token_data = self.token(data)\n",
    "        filter_data = self.filtering(token_data)\n",
    "        stem_data = self.stem(filter_data)\n",
    "        return stem_data\n",
    "    \n",
    "    def getTerm(self, text:list):\n",
    "        output=[]\n",
    "        for i in range(len(text)):\n",
    "            for j in range(len(text[i])):\n",
    "                if text[i][j] not in output:\n",
    "                    output.append(text[i][j])\n",
    "        return output\n",
    "    \n",
    "    def translate(self,text:list):\n",
    "        trans = Translator(service_urls=['translate.google.com',\"translate.google.co.id\"])\n",
    "        output = []\n",
    "        for i in range(len(text)):\n",
    "            sentence = text[i]\n",
    "            is_arr = isinstance(sentence, list)\n",
    "            if is_arr:\n",
    "                sentence = \" \".join(sentence)\n",
    "            temp = trans.translate(sentence, dest='id').text\n",
    "            if is_arr:\n",
    "                temp = temp.split(\" \")\n",
    "            output.append(temp)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extraction:\n",
    "    def __init__(self):\n",
    "        self.__data = []\n",
    "        \n",
    "    def getTF(self,text:list, term:list, tfWeight=True):\n",
    "        output = []\n",
    "        if not (isinstance(text, list) or isinstance(text, np.array)):\n",
    "            raise \" \"\n",
    "        for i in range(len(text)):\n",
    "            doc = text[i]\n",
    "            if isinstance(text[i], list):\n",
    "                doc = \" \".join(doc)\n",
    "            tf_doc = []\n",
    "            for j in range(len(term)):\n",
    "                temp = doc.count(term[j])\n",
    "                tf_doc.append(temp)\n",
    "            output.append(tf_doc)\n",
    "        output = np.array(output)\n",
    "        #w_tf=1+log(tf) tf>0, tf=0: 0\n",
    "        if tfWeight:\n",
    "            output = np.where(output>0,1+np.log10(output),0)\n",
    "        return output\n",
    "\n",
    "    def getIDF(self,tf_doc:list):\n",
    "        # makan | minum | tidur\n",
    "        # 1     | 0     | 1      \n",
    "        # 0     | 0     | 0\n",
    "        countNZero = np.count_nonzero(tf_doc, axis=0)\n",
    "        n_doc = tf_doc.shape[0]\n",
    "        return np.where(countNZero != 0 ,np.log10(n_doc/countNZero),0)\n",
    "\n",
    "    def __normalized_vector(self,tfidf, train=False):\n",
    "        def get_length(tfidf):\n",
    "            pow_tfidf = np.power(tfidf,2)\n",
    "            sum_tfidf = np.sum(pow_tfidf, axis=1)\n",
    "            return np.sqrt(sum_tfidf)\n",
    "        length_tfidf = get_length(tfidf)\n",
    "        norm = tfidf.T/np.where(length_tfidf==0,1,length_tfidf)\n",
    "        where_are_NaNs = np.isnan(norm)\n",
    "        norm[where_are_NaNs] = 0\n",
    "        return norm.T\n",
    "    \n",
    "    def getTFIDF(self,tf, idf):\n",
    "        tfidf = tf*idf\n",
    "        return self.__normalized_vector(tfidf,True)\n",
    "\n",
    "class knn:\n",
    "    def __init__(self):\n",
    "        self.__data = []\n",
    "        self.__target = []\n",
    "    \n",
    "    def fit(self, x:list,y:list):\n",
    "        if x.ndim!=2:\n",
    "            raise \"Error data Train is not 2dimention\"\n",
    "        self.__data = x\n",
    "        self.__target = y\n",
    "    \n",
    "    def __cossim(self,tfidf_train, tfidf_test):\n",
    "        if tfidf_test.ndim == 2:\n",
    "            res = []\n",
    "            for i in range(tfidf_test.shape[0]):\n",
    "                res.append(np.dot(tfidf_train, tfidf_test[i]))\n",
    "            return np.array(res)\n",
    "        return np.array([np.dot(tfidf_train, tfidf_test)])\n",
    "    \n",
    "    def predict(self, x_norm:list, k:int=3, norm=True):\n",
    "        if len(self.__data)==0:\n",
    "            raise \"Error please input data train\"\n",
    "        result = self.__cossim(self.__data, x_norm)\n",
    "        target = []\n",
    "        for i in range(len(result)):\n",
    "            hasil = sorted(zip(result[i],self.__target), key=lambda pair: pair[0], reverse=True)[:k]\n",
    "            hsl = dict(Counter([j for i,j in hasil]))\n",
    "            if hsl.get(1,0) > hsl.get(-1,0):\n",
    "                target.append(1)\n",
    "            elif hsl.get(1,0) < hsl.get(-1,0):\n",
    "                target.append(-1)\n",
    "            elif hsl.get(\"1\",0) > hsl.get(\"-1\",0):\n",
    "                target.append(1)\n",
    "            elif hsl.get(\"1\",0) < hsl.get(\"-1\",0):\n",
    "                target.append(-1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read = pd.read_csv(\"./data/tests/posindonesia_kualitas.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kecepatan_tiki = kecepatan_data.loc[kecepatan_data.loc[:,\"jasa\"] ==\"tiki\",]\n",
    "#kecepatan_jne = kecepatan_data.loc[kecepatan_data.loc[:,\"jasa\"] ==\"jne\",]\n",
    "data = data_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-5dede309cec5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# result_prep = prep.prep_data(list(data.tweet))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# term = prep.getTerm(result_prep)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-0a740aed79fe>\u001b[0m in \u001b[0;36mprep_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprep_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveLink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mtoken_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mfilter_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiltering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-0a740aed79fe>\u001b[0m in \u001b[0;36mremoveLink\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "result_prep = prep.prep_data(list(data.tweet))\n",
    "term = prep.getTerm(result_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = extraction()\n",
    "tf = ex.getTF(result_prep, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = ex.getIDF(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.21981901, 0.09820078, 0.11805237, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.51952328],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "tf_idf = ex.getTFIDF(tf,idf)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf, data.loc[:,[\"jasa\",\"sentimen\"]], test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = knn()\n",
    "knn_model.fit(X_train, y_train.sentimen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_model.predict(X_test,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matriks = confusion_matrix(y_test.sentimen, y_pred)\n",
    "conf_matriks\n",
    "# conf_matriks.shape\n",
    "percentage_positive = conf_matriks[0][0]/(np.sum(conf_matriks))\n",
    "if (conf_matriks.shape == (2, 2)):\n",
    "    percentage_negative = conf_matriks[1][1]/(np.sum(conf_matriks))\n",
    "else:\n",
    "    percentage_negative = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.9555555555555556, 0.0)"
      ]
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "percentage_positive, percentage_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "precision_score(y_test.sentimen, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "recall_score(y_test.sentimen, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[86,  0],\n",
       "       [ 4,  0]])"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "conf_matriks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             jasa  sentimen\n",
       "281  PosIndonesia        -1\n",
       "265  PosIndonesia        -1\n",
       "164  PosIndonesia        -1\n",
       "9    PosIndonesia        -1\n",
       "77   PosIndonesia        -1\n",
       "..            ...       ...\n",
       "132  PosIndonesia        -1\n",
       "72   PosIndonesia        -1\n",
       "15   PosIndonesia        -1\n",
       "10   PosIndonesia        -1\n",
       "157  PosIndonesia        -1\n",
       "\n",
       "[90 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>jasa</th>\n      <th>sentimen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>281</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>265</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>157</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>90 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             jasa  sentimen\n",
       "224  PosIndonesia        -1\n",
       "68   PosIndonesia        -1\n",
       "222  PosIndonesia        -1\n",
       "37   PosIndonesia        -1\n",
       "16   PosIndonesia        -1\n",
       "..            ...       ...\n",
       "188  PosIndonesia        -1\n",
       "71   PosIndonesia        -1\n",
       "106  PosIndonesia        -1\n",
       "270  PosIndonesia         1\n",
       "102  PosIndonesia        -1\n",
       "\n",
       "[209 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>jasa</th>\n      <th>sentimen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>224</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>222</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>270</th>\n      <td>PosIndonesia</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>PosIndonesia</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>209 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}