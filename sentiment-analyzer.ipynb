{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scorecardpy as sc\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from googletrans import Translator, LANGUAGES\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "from googletrans import Translator, LANGUAGES\n",
    "from imblearn.combine import SMOTEENN\n",
    "np.seterr(divide = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "    def __init__(self):\n",
    "        def normalisasi(norm):\n",
    "            dt = {}\n",
    "            for i in range(len(norm)):\n",
    "                dt[norm.loc[i,\"kata typo\"]] = norm.loc[i,\"kata normal\"]\n",
    "            return dt\n",
    "        self.__kamusNorm = normalisasi(pd.read_csv(\"./data/components/word-normalization.csv\", delimiter=\";\"))\n",
    "        self.__stopword = list(pd.read_csv(\"./data/components/stopword.csv\",header=None).loc[:,0])\n",
    "\n",
    "    def token(self,text:list):\n",
    "        output = []\n",
    "        for i in range(len(text)):\n",
    "            output.append(text[i].split(\" \"))\n",
    "        return output\n",
    "    \n",
    "    def stem(self,text: list):\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "\n",
    "        output = []\n",
    "        # stemming process\n",
    "        for i in range(len(text)):\n",
    "            sentence = text[i]\n",
    "            is_list = False\n",
    "            if isinstance(sentence, list):\n",
    "                sentence = \" \".join(sentence)\n",
    "                is_list = True\n",
    "            temp   = stemmer.stem(sentence)\n",
    "            if is_list:\n",
    "                temp = temp.split(\" \")\n",
    "            output.append(temp)\n",
    "        return output\n",
    "\n",
    "    def filtering(self, text: list):\n",
    "        output = []\n",
    "        for i in range(len(text)):\n",
    "            temp = []\n",
    "            for j in range(len(text[i])):\n",
    "                if text[i][j] not in self.__stopword:\n",
    "                    temp.append(text[i][j])\n",
    "            output.append(temp)\n",
    "        return output\n",
    "    \n",
    "    def removeLink(self, data:list):\n",
    "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "        for i in range(len(data)):\n",
    "            data[i] = re.sub(regex, \" \", data[i])\n",
    "        return data\n",
    "    \n",
    "    def normalization(self,data):\n",
    "        length_data = np.array(data).shape\n",
    "        for i in range(length_data[0]):\n",
    "            is_str= isinstance(data[i], str)\n",
    "            temp = data[i]\n",
    "            if is_str:\n",
    "                temp = data[i].split(\" \")\n",
    "                \n",
    "            for j in range(len(temp)):\n",
    "                temp[j] = self.__kamusNorm.get(temp[j],temp[j])\n",
    "            if is_str:\n",
    "                temp = \" \".join(temp)\n",
    "            data[i] = temp\n",
    "        return data\n",
    "    \n",
    "    def prep_data(self,data):\n",
    "        data = self.removeLink(data)\n",
    "        translate_data = self.translate(data)\n",
    "        token_data = self.token(translate_data)\n",
    "        filter_data = self.filtering(token_data)\n",
    "        stem_data = self.stem(filter_data)\n",
    "        return stem_data\n",
    "    \n",
    "    def getTerm(self, text:list):\n",
    "        output=[]\n",
    "        for i in range(len(text)):\n",
    "            for j in range(len(text[i])):\n",
    "                if text[i][j] not in output:\n",
    "                    output.append(text[i][j])\n",
    "        return output\n",
    "    \n",
    "    def translate(self,text:list):\n",
    "        trans = Translator(service_urls=['translate.google.com',\"translate.google.co.id\"])\n",
    "        output = []\n",
    "        for i in range(len(text)):           \n",
    "            sentence = text[i]\n",
    "            is_arr = isinstance(sentence, list)\n",
    "            if is_arr:\n",
    "                sentence = \" \".join(sentence)\n",
    "            temp = trans.translate(sentence, dest='id').text\n",
    "            if is_arr:\n",
    "                temp = temp.split(\" \")\n",
    "            output.append(temp)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extraction:\n",
    "    def __init__(self):\n",
    "        self.__data = []\n",
    "        \n",
    "    def getTF(self,text:list, term:list, tfWeight=True):\n",
    "        output = []\n",
    "        if not (isinstance(text, list) or isinstance(text, np.array)):\n",
    "            raise \" \"\n",
    "        for i in range(len(text)):\n",
    "            doc = text[i]\n",
    "            if isinstance(text[i], list):\n",
    "                doc = \" \".join(doc)\n",
    "            tf_doc = []\n",
    "            for j in range(len(term)):\n",
    "                temp = doc.count(term[j])\n",
    "                tf_doc.append(temp)\n",
    "            output.append(tf_doc)\n",
    "        output = np.array(output)\n",
    "        #w_tf=1+log(tf) tf>0, tf=0: 0\n",
    "        if tfWeight:\n",
    "            output = np.where(output>0,1+np.log10(output),0)\n",
    "        return output\n",
    "\n",
    "    def getIDF(self,tf_doc:list):\n",
    "        # makan | minum | tidur\n",
    "        # 1     | 0     | 1      \n",
    "        # 0     | 0     | 0\n",
    "        countNZero = np.count_nonzero(tf_doc, axis=0)\n",
    "        n_doc = tf_doc.shape[0]\n",
    "        return np.where(countNZero != 0 ,np.log10(n_doc/countNZero),0)\n",
    "\n",
    "    def __normalized_vector(self,tfidf, train=False):\n",
    "        def get_length(tfidf):\n",
    "            pow_tfidf = np.power(tfidf,2)\n",
    "            sum_tfidf = np.sum(pow_tfidf, axis=1)\n",
    "            return np.sqrt(sum_tfidf)\n",
    "        length_tfidf = get_length(tfidf)\n",
    "        norm = tfidf.T/np.where(length_tfidf==0,1,length_tfidf)\n",
    "        where_are_NaNs = np.isnan(norm)\n",
    "        norm[where_are_NaNs] = 0\n",
    "        return norm.T\n",
    "    \n",
    "    def getTFIDF(self,tf, idf):\n",
    "        tfidf = tf*idf\n",
    "        return self.__normalized_vector(tfidf,True)\n",
    "\n",
    "class knn:\n",
    "    def __init__(self):\n",
    "        self.__data = []\n",
    "        self.__target = []\n",
    "    \n",
    "    def fit(self, x:list,y:list):\n",
    "        if x.ndim!=2:\n",
    "            raise \"Error data Train is not 2dimention\"\n",
    "        self.__data = x\n",
    "        self.__target = y\n",
    "    \n",
    "    def __cossim(self, tfidf_train, tfidf_test):\n",
    "        if tfidf_test.ndim == 2:\n",
    "            res = []\n",
    "            for i in range(tfidf_test.shape[0]):\n",
    "                res.append(np.dot(tfidf_train, tfidf_test[i]))\n",
    "            return np.array(res)\n",
    "        return np.array([np.dot(tfidf_train, tfidf_test)])\n",
    "    \n",
    "    def predict(self, x_norm:list, k:int=3, norm=True):\n",
    "        if len(self.__data)==0:\n",
    "            raise \"Error please input data train\"\n",
    "        result = self.__cossim(self.__data, x_norm)\n",
    "        target = []\n",
    "        for i in range(len(result)):\n",
    "            hasil = sorted(zip(result[i],self.__target), key=lambda pair: pair[0], reverse=True)[:k]\n",
    "            hsl = dict(Counter([j for i,j in hasil]))\n",
    "            if hsl.get(1,0) > hsl.get(-1,0):\n",
    "                target.append(1)\n",
    "            elif hsl.get(1,0) < hsl.get(-1,0):\n",
    "                target.append(-1)\n",
    "            elif hsl.get(\"1\",0) > hsl.get(\"-1\",0):\n",
    "                target.append(1)\n",
    "            elif hsl.get(\"1\",0) < hsl.get(\"-1\",0):\n",
    "                target.append(-1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIVnew(tfidf, term,target):\n",
    "    df = pd.DataFrame(tfidf,columns=term)\n",
    "    df = df.assign(sentimen_i = target)\n",
    "    info_val = sc.iv(df, y=\"sentimen_i\")\n",
    "    return info_val\n",
    "    \n",
    "def feature_selection(term, iv, select = 0.02):\n",
    "    feature_selections = iv.variable[iv.info_value>=select]\n",
    "    index_feature = []\n",
    "    for i in feature_selections:\n",
    "        terms = term.index(i)\n",
    "        index_feature.append(terms)\n",
    "    return index_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_read = pd.read_csv(\"./data/tests/test.csv\", delimiter=\";\")\n",
    "# data_read_train = pd.read_csv(\"./data/tests/train.csv\", delimiter=\";\")\n",
    "# data_read_test = pd.read_csv(\"./data/tests/test.csv\", delimiter=\";\")\n",
    "data_read_train = pd.read_csv(\"./data/tests/tests-combine-train/train_data_kecepatan.csv\", delimiter=\";\")\n",
    "data_train = data_read_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read_test = pd.read_csv(\"./data/tests/tests-combine-train/test_data_kecepatan.csv\", delimiter=\";\")\n",
    "data_test = data_read_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_prep_train = prep.prep_data(list(data_train.tweet))\n",
    "result_prep_test = prep.prep_data(list(data_test.tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_train = prep.getTerm(result_prep_train)\n",
    "term_test = prep.getTerm(result_prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'term_train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a6d86b62b657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mterm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_train\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mterm_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_prep_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_prep_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'term_train' is not defined"
     ]
    }
   ],
   "source": [
    "ex = extraction()\n",
    "term = list(set(term_train+term_test))\n",
    "tf_train = ex.getTF(result_prep_train, term)\n",
    "tf_test = ex.getTF(result_prep_test, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = ex.getIDF(np.concatenate((tf_train, tf_test), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = ex.getTFIDF(tf_train, idf)\n",
    "tf_idf_test = ex.getTFIDF(tf_test, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.concatenate((tf_idf_test, tf_idf_train), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(811,)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# np.concatenate((data_test.sentimen, data_train.sentimen), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((30, 394), (9, 394))"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# tf_idf_train.shape tf_idf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.concatenate((data_train.sentimen, data_test.sentimen), axis=0)\n",
    "concat_tfidf = np.concatenate((tf_idf_test, tf_idf_train), axis=0)\n",
    "information_value = getIVnew(concat_tfidf, term, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(39, 394)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# concat_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeks = feature_selection(term, information_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(tf_idf_train, data_train.loc[:,[\"jasa\",\"sentimen\"]], test_size=1, random_state=42)\n",
    "# X_train, y_train = train_test_split(tf_idf_train, data_train.loc[:,[\"jasa\",\"sentimen\"]], test_size=1, random_state=42)\n",
    "# X_test, y_test = train_test_split(tf_idf_test, data_test.loc[:,[\"jasa\",\"sentimen\"]], test_size=1, random_state=42)\n",
    "X_train = tf_idf_train[:,indeks]\n",
    "X_test = tf_idf_test[:,indeks]\n",
    "y_train = data_train.loc[:,[\"jasa\",\"sentimen\"]]\n",
    "y_test = data_test.loc[:,[\"jasa\",\"sentimen\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTEENN()\n",
    "X_data_sampling, y_data_sampling = sm.fit_resample(X_train, y_train.sentimen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = knn()\n",
    "knn_model.fit(X_train, y_train.sentimen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = knn_model.predict(X_test,k=3)\n",
    "y_pred = knn_model.predict(X_test,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_arr = np.array(y_pred)\n",
    "sum_of_negative = np.count_nonzero(y_pred_arr == -1)\n",
    "sum_of_positive = np.count_nonzero(y_pred_arr == 1)\n",
    "prcentage_negative = sum_of_negative/len(y_pred_arr)*100\n",
    "percentage_positive = sum_of_positive/len(y_pred_arr)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1.8633540372670807, 98.13664596273291)"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "percentage_positive, prcentage_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1]),\n",
       " array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1]))"
      ]
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "confusion_matrix(np.array(y_test.sentimen), np.array(y_pred)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(np.array(y_test.sentimen), np.array(y_pred)).ravel()\n",
    "conf_matriks = confusion_matrix(np.array(y_test.sentimen), np.array(y_pred))\n",
    "# conf_matriks.shape\n",
    "conf_matriks_positive = tp/(np.sum([[90, 0], [0, 0]]))\n",
    "conf_matriks_negative = tn/(np.sum([[90, 0], [0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[158,   3],\n",
       "       [  0,   0]])"
      ]
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "conf_matriks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(158, 3, 0, 0)"
      ]
     },
     "metadata": {},
     "execution_count": 168
    }
   ],
   "source": [
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 1,  1, -1, -1, -1,  1,  1,  1, -1, -1])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "np.array(y_test.sentimen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 1, -1,  1,  1, -1, -1, -1, -1,  1, -1])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.03333333333333333, 0.9333333333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "conf_matriks_positive, conf_matriks_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision =  precision_score(y_test.sentimen, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(y_test.sentimen, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_measure = 2 * ( (precision * recall) / (precision + recall) )\n",
    "# f_measure = 2*tp / ((tp+fp) * (tp+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp+tn)/(tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9813664596273292"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}